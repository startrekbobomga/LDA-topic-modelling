{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READING FILE<PreProcessing>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darji\\AppData\\Local\\Temp\\ipykernel_26672\\1771419806.py:3: DtypeWarning: Columns (13,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"hydrogen fuel data.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "df = pd.read_csv(\"hydrogen fuel data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = df[[\"Link Text\"]]\n",
    "data_text.loc[:, 'index'] = df.index\n",
    "documents = data_text\n",
    "pd.DataFrame(documents)\n",
    "documents.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Photos', 'from', 'Ayr', 'State', 'High', \"School's\", 'post']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['photo', 'state', 'high', 'school', 'post']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs= documents.dropna(subset = ['Link Text'],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['Link Text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictonary of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<0 unique tokens: []>\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (26, 1), (251, 1)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"photo\") appears 1 time.\n",
      "Word 26 (\"post\") appears 1 time.\n",
      "Word 251 (\"quantron\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5541679690417858), (1, 0.8324048666893427)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.092*\"hydrogen\" + 0.059*\"fuel\" + 0.031*\"power\" + 0.019*\"truck\" + 0.017*\"green\" + 0.017*\"cell\" + 0.014*\"clean\" + 0.013*\"energi\" + 0.012*\"fossil\" + 0.012*\"news\"\n",
      "Topic: 1 \n",
      "Words: 0.088*\"hydrogen\" + 0.046*\"fuel\" + 0.040*\"electr\" + 0.029*\"vehicl\" + 0.025*\"cell\" + 0.025*\"power\" + 0.015*\"futur\" + 0.015*\"truck\" + 0.014*\"car\" + 0.011*\"massiv\"\n",
      "Topic: 2 \n",
      "Words: 0.127*\"hydrogen\" + 0.080*\"fuel\" + 0.055*\"cell\" + 0.028*\"power\" + 0.021*\"develop\" + 0.013*\"life\" + 0.013*\"produc\" + 0.013*\"plant\" + 0.012*\"crude\" + 0.011*\"india\"\n",
      "Topic: 3 \n",
      "Words: 0.063*\"fuel\" + 0.063*\"launch\" + 0.055*\"nasa\" + 0.054*\"hydrogen\" + 0.043*\"moon\" + 0.043*\"rocket\" + 0.042*\"leak\" + 0.030*\"attempt\" + 0.023*\"artemi\" + 0.019*\"power\"\n",
      "Topic: 4 \n",
      "Words: 0.057*\"hydrogen\" + 0.028*\"energi\" + 0.027*\"post\" + 0.026*\"photo\" + 0.023*\"green\" + 0.017*\"emiss\" + 0.016*\"power\" + 0.015*\"electr\" + 0.014*\"carbon\" + 0.014*\"zero\"\n",
      "Topic: 5 \n",
      "Words: 0.096*\"hydrogen\" + 0.047*\"power\" + 0.035*\"energi\" + 0.029*\"clean\" + 0.028*\"climat\" + 0.023*\"fuel\" + 0.021*\"green\" + 0.018*\"engin\" + 0.017*\"carbon\" + 0.013*\"research\"\n",
      "Topic: 6 \n",
      "Words: 0.213*\"photo\" + 0.118*\"timelin\" + 0.098*\"post\" + 0.048*\"hydrogen\" + 0.038*\"energi\" + 0.026*\"fuel\" + 0.024*\"transit\" + 0.020*\"turbin\" + 0.020*\"lead\" + 0.019*\"asia\"\n",
      "Topic: 7 \n",
      "Words: 0.085*\"hydrogen\" + 0.072*\"green\" + 0.032*\"project\" + 0.031*\"energi\" + 0.024*\"power\" + 0.023*\"india\" + 0.020*\"news\" + 0.016*\"sign\" + 0.013*\"fuel\" + 0.012*\"industri\"\n",
      "Topic: 8 \n",
      "Words: 0.084*\"hydrogen\" + 0.046*\"fuel\" + 0.026*\"cell\" + 0.025*\"toyota\" + 0.024*\"power\" + 0.019*\"test\" + 0.018*\"vehicl\" + 0.018*\"work\" + 0.015*\"share\" + 0.015*\"energi\"\n",
      "Topic: 9 \n",
      "Words: 0.073*\"hydrogen\" + 0.043*\"green\" + 0.042*\"power\" + 0.031*\"energi\" + 0.027*\"renew\" + 0.024*\"plant\" + 0.020*\"tech\" + 0.018*\"announc\" + 0.016*\"compani\" + 0.015*\"plan\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.195*\"timelin\" + 0.150*\"photo\" + 0.032*\"post\" + 0.016*\"share\" + 0.016*\"energi\" + 0.015*\"asia\" + 0.015*\"pimagazin\" + 0.015*\"bangladesh\" + 0.014*\"lead\" + 0.014*\"turbin\"\n",
      "Topic: 1 Word: 0.030*\"fuel\" + 0.026*\"launch\" + 0.024*\"nasa\" + 0.021*\"rocket\" + 0.021*\"moon\" + 0.020*\"leak\" + 0.018*\"hydrogen\" + 0.018*\"cell\" + 0.012*\"futur\" + 0.012*\"ruin\"\n",
      "Topic: 2 Word: 0.022*\"climat\" + 0.022*\"solut\" + 0.021*\"ocean\" + 0.019*\"research\" + 0.018*\"california\" + 0.017*\"power\" + 0.017*\"attempt\" + 0.016*\"hydrogen\" + 0.014*\"energi\" + 0.012*\"photo\"\n",
      "Topic: 3 Word: 0.021*\"hydrogen\" + 0.019*\"fuel\" + 0.015*\"energi\" + 0.012*\"green\" + 0.012*\"power\" + 0.010*\"fossil\" + 0.010*\"engin\" + 0.009*\"timelin\" + 0.009*\"cell\" + 0.008*\"batteri\"\n",
      "Topic: 4 Word: 0.022*\"power\" + 0.021*\"post\" + 0.020*\"photo\" + 0.017*\"hydrogen\" + 0.013*\"fuel\" + 0.012*\"energi\" + 0.010*\"green\" + 0.010*\"plan\" + 0.010*\"test\" + 0.009*\"news\"\n",
      "Topic: 5 Word: 0.024*\"hydrogen\" + 0.018*\"green\" + 0.015*\"fuel\" + 0.015*\"cell\" + 0.014*\"power\" + 0.013*\"energi\" + 0.012*\"india\" + 0.011*\"electr\" + 0.011*\"stori\" + 0.010*\"truck\"\n",
      "Topic: 6 Word: 0.017*\"hydrogen\" + 0.016*\"green\" + 0.014*\"fuel\" + 0.013*\"engin\" + 0.013*\"power\" + 0.011*\"announc\" + 0.010*\"africa\" + 0.010*\"tech\" + 0.010*\"https\" + 0.010*\"compani\"\n",
      "Topic: 7 Word: 0.098*\"post\" + 0.083*\"photo\" + 0.015*\"hydrogen\" + 0.012*\"hyundai\" + 0.011*\"fuel\" + 0.011*\"power\" + 0.011*\"citi\" + 0.010*\"energi\" + 0.009*\"green\" + 0.009*\"cell\"\n",
      "Topic: 8 Word: 0.019*\"hydrogen\" + 0.017*\"power\" + 0.017*\"green\" + 0.016*\"energi\" + 0.014*\"fuel\" + 0.013*\"clean\" + 0.012*\"sign\" + 0.011*\"develop\" + 0.009*\"deal\" + 0.009*\"ammonia\"\n",
      "Topic: 9 Word: 0.029*\"hydrogen\" + 0.017*\"timelin\" + 0.017*\"fuel\" + 0.016*\"cell\" + 0.015*\"photo\" + 0.014*\"power\" + 0.014*\"green\" + 0.012*\"plant\" + 0.012*\"news\" + 0.012*\"life\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['photo', 'state', 'high', 'school', 'post']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7749659419059753\t \n",
      "Topic: 0.213*\"photo\" + 0.118*\"timelin\" + 0.098*\"post\" + 0.048*\"hydrogen\" + 0.038*\"energi\" + 0.026*\"fuel\" + 0.024*\"transit\" + 0.020*\"turbin\" + 0.020*\"lead\" + 0.019*\"asia\"\n",
      "\n",
      "Score: 0.025012921541929245\t \n",
      "Topic: 0.092*\"hydrogen\" + 0.059*\"fuel\" + 0.031*\"power\" + 0.019*\"truck\" + 0.017*\"green\" + 0.017*\"cell\" + 0.014*\"clean\" + 0.013*\"energi\" + 0.012*\"fossil\" + 0.012*\"news\"\n",
      "\n",
      "Score: 0.0250038281083107\t \n",
      "Topic: 0.063*\"fuel\" + 0.063*\"launch\" + 0.055*\"nasa\" + 0.054*\"hydrogen\" + 0.043*\"moon\" + 0.043*\"rocket\" + 0.042*\"leak\" + 0.030*\"attempt\" + 0.023*\"artemi\" + 0.019*\"power\"\n",
      "\n",
      "Score: 0.025003353133797646\t \n",
      "Topic: 0.057*\"hydrogen\" + 0.028*\"energi\" + 0.027*\"post\" + 0.026*\"photo\" + 0.023*\"green\" + 0.017*\"emiss\" + 0.016*\"power\" + 0.015*\"electr\" + 0.014*\"carbon\" + 0.014*\"zero\"\n",
      "\n",
      "Score: 0.025002496317029\t \n",
      "Topic: 0.084*\"hydrogen\" + 0.046*\"fuel\" + 0.026*\"cell\" + 0.025*\"toyota\" + 0.024*\"power\" + 0.019*\"test\" + 0.018*\"vehicl\" + 0.018*\"work\" + 0.015*\"share\" + 0.015*\"energi\"\n",
      "\n",
      "Score: 0.02500234916806221\t \n",
      "Topic: 0.085*\"hydrogen\" + 0.072*\"green\" + 0.032*\"project\" + 0.031*\"energi\" + 0.024*\"power\" + 0.023*\"india\" + 0.020*\"news\" + 0.016*\"sign\" + 0.013*\"fuel\" + 0.012*\"industri\"\n",
      "\n",
      "Score: 0.025002330541610718\t \n",
      "Topic: 0.073*\"hydrogen\" + 0.043*\"green\" + 0.042*\"power\" + 0.031*\"energi\" + 0.027*\"renew\" + 0.024*\"plant\" + 0.020*\"tech\" + 0.018*\"announc\" + 0.016*\"compani\" + 0.015*\"plan\"\n",
      "\n",
      "Score: 0.02500228025019169\t \n",
      "Topic: 0.088*\"hydrogen\" + 0.046*\"fuel\" + 0.040*\"electr\" + 0.029*\"vehicl\" + 0.025*\"cell\" + 0.025*\"power\" + 0.015*\"futur\" + 0.015*\"truck\" + 0.014*\"car\" + 0.011*\"massiv\"\n",
      "\n",
      "Score: 0.025002261623740196\t \n",
      "Topic: 0.096*\"hydrogen\" + 0.047*\"power\" + 0.035*\"energi\" + 0.029*\"clean\" + 0.028*\"climat\" + 0.023*\"fuel\" + 0.021*\"green\" + 0.018*\"engin\" + 0.017*\"carbon\" + 0.013*\"research\"\n",
      "\n",
      "Score: 0.025002235546708107\t \n",
      "Topic: 0.127*\"hydrogen\" + 0.080*\"fuel\" + 0.055*\"cell\" + 0.028*\"power\" + 0.021*\"develop\" + 0.013*\"life\" + 0.013*\"produc\" + 0.013*\"plant\" + 0.012*\"crude\" + 0.011*\"india\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7749714851379395\t \n",
      "Topic: 0.098*\"post\" + 0.083*\"photo\" + 0.015*\"hydrogen\" + 0.012*\"hyundai\" + 0.011*\"fuel\" + 0.011*\"power\" + 0.011*\"citi\" + 0.010*\"energi\" + 0.009*\"green\" + 0.009*\"cell\"\n",
      "\n",
      "Score: 0.025010421872138977\t \n",
      "Topic: 0.195*\"timelin\" + 0.150*\"photo\" + 0.032*\"post\" + 0.016*\"share\" + 0.016*\"energi\" + 0.015*\"asia\" + 0.015*\"pimagazin\" + 0.015*\"bangladesh\" + 0.014*\"lead\" + 0.014*\"turbin\"\n",
      "\n",
      "Score: 0.025003336369991302\t \n",
      "Topic: 0.019*\"hydrogen\" + 0.017*\"power\" + 0.017*\"green\" + 0.016*\"energi\" + 0.014*\"fuel\" + 0.013*\"clean\" + 0.012*\"sign\" + 0.011*\"develop\" + 0.009*\"deal\" + 0.009*\"ammonia\"\n",
      "\n",
      "Score: 0.025003068149089813\t \n",
      "Topic: 0.029*\"hydrogen\" + 0.017*\"timelin\" + 0.017*\"fuel\" + 0.016*\"cell\" + 0.015*\"photo\" + 0.014*\"power\" + 0.014*\"green\" + 0.012*\"plant\" + 0.012*\"news\" + 0.012*\"life\"\n",
      "\n",
      "Score: 0.02500264346599579\t \n",
      "Topic: 0.030*\"fuel\" + 0.026*\"launch\" + 0.024*\"nasa\" + 0.021*\"rocket\" + 0.021*\"moon\" + 0.020*\"leak\" + 0.018*\"hydrogen\" + 0.018*\"cell\" + 0.012*\"futur\" + 0.012*\"ruin\"\n",
      "\n",
      "Score: 0.025002358481287956\t \n",
      "Topic: 0.022*\"power\" + 0.021*\"post\" + 0.020*\"photo\" + 0.017*\"hydrogen\" + 0.013*\"fuel\" + 0.012*\"energi\" + 0.010*\"green\" + 0.010*\"plan\" + 0.010*\"test\" + 0.009*\"news\"\n",
      "\n",
      "Score: 0.02500191144645214\t \n",
      "Topic: 0.022*\"climat\" + 0.022*\"solut\" + 0.021*\"ocean\" + 0.019*\"research\" + 0.018*\"california\" + 0.017*\"power\" + 0.017*\"attempt\" + 0.016*\"hydrogen\" + 0.014*\"energi\" + 0.012*\"photo\"\n",
      "\n",
      "Score: 0.02500181458890438\t \n",
      "Topic: 0.017*\"hydrogen\" + 0.016*\"green\" + 0.014*\"fuel\" + 0.013*\"engin\" + 0.013*\"power\" + 0.011*\"announc\" + 0.010*\"africa\" + 0.010*\"tech\" + 0.010*\"https\" + 0.010*\"compani\"\n",
      "\n",
      "Score: 0.025001564994454384\t \n",
      "Topic: 0.024*\"hydrogen\" + 0.018*\"green\" + 0.015*\"fuel\" + 0.015*\"cell\" + 0.014*\"power\" + 0.013*\"energi\" + 0.012*\"india\" + 0.011*\"electr\" + 0.011*\"stori\" + 0.010*\"truck\"\n",
      "\n",
      "Score: 0.025001410394906998\t \n",
      "Topic: 0.021*\"hydrogen\" + 0.019*\"fuel\" + 0.015*\"energi\" + 0.012*\"green\" + 0.012*\"power\" + 0.010*\"fossil\" + 0.010*\"engin\" + 0.009*\"timelin\" + 0.009*\"cell\" + 0.008*\"batteri\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.36961957812309265\t Topic: 0.092*\"hydrogen\" + 0.059*\"fuel\" + 0.031*\"power\" + 0.019*\"truck\" + 0.017*\"green\"\n",
      "Score: 0.21131421625614166\t Topic: 0.127*\"hydrogen\" + 0.080*\"fuel\" + 0.055*\"cell\" + 0.028*\"power\" + 0.021*\"develop\"\n",
      "Score: 0.17109526693820953\t Topic: 0.096*\"hydrogen\" + 0.047*\"power\" + 0.035*\"energi\" + 0.029*\"clean\" + 0.028*\"climat\"\n",
      "Score: 0.0901322290301323\t Topic: 0.088*\"hydrogen\" + 0.046*\"fuel\" + 0.040*\"electr\" + 0.029*\"vehicl\" + 0.025*\"cell\"\n",
      "Score: 0.06326694041490555\t Topic: 0.073*\"hydrogen\" + 0.043*\"green\" + 0.042*\"power\" + 0.031*\"energi\" + 0.027*\"renew\"\n",
      "Score: 0.058922797441482544\t Topic: 0.084*\"hydrogen\" + 0.046*\"fuel\" + 0.026*\"cell\" + 0.025*\"toyota\" + 0.024*\"power\"\n",
      "Score: 0.03422674536705017\t Topic: 0.085*\"hydrogen\" + 0.072*\"green\" + 0.032*\"project\" + 0.031*\"energi\" + 0.024*\"power\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = input()\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.32472290323498537\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs,dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence Score: 0.35174063669427047\n"
     ]
    }
   ],
   "source": [
    "coherence_model_TFIDF = CoherenceModel(model=lda_model_tfidf,texts=processed_docs,coherence='c_v')\n",
    "coherence_model_TFIDF = coherence_model_TFIDF.get_coherence()\n",
    "print('coherence Score:',coherence_model_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/540 [00:14<?, ?it/s]\n",
      "100%|██████████| 540/540 [2:03:00<00:00, 13.67s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(bow_corpus)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(bow_corpus, int(num_of_docs*0.75)), \n",
    "               bow_corpus]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i],dictionary=dictionary,\n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close() \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Visualize the topics\u001b[39;00m\n\u001b[0;32m      6\u001b[0m pyLDAvis\u001b[39m.\u001b[39menable_notebook()\n\u001b[1;32m----> 8\u001b[0m LDAvis_data_filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m./results/ldavis_tuned_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(num_topics))\n\u001b[0;32m     10\u001b[0m \u001b[39m# # this is a bit time consuming - make the if statement True\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m# # if you want to execute visualization prep yourself\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
